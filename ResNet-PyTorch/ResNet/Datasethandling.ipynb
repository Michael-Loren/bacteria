{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f917bdb6-118b-4a25-ac24-b150ff6c72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import scipy.io\n",
    "import torch.nn as nn  # Import nn module\n",
    "import torch.optim as optim  # Import optim module\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperspectral_dataset import HyperspectralDataset, compute_statistics, NormalizeCube, NormalizeProfile\n",
    "\n",
    "from ResNet import CustomResNet50  # Assuming ResNet50 is defined in ResNet.py\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c172e4-ee83-47e4-aba8-efca02e99daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccab103-2289-4111-bf41-0447ae956224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n"
     ]
    }
   ],
   "source": [
    "# Function to get all file paths from a dataset\n",
    "def get_all_file_paths(dataset):\n",
    "    file_paths = []\n",
    "    for idx in range(len(dataset)):\n",
    "        file_paths.append(dataset.get_image_path(idx))\n",
    "    return file_paths\n",
    "\n",
    "# # Initialize the dataset with transformations\n",
    "dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_all')\n",
    "channel_means_cube, channel_stds_cube, channel_means_profile, channel_stds_profile = compute_statistics(dataset)\n",
    "\n",
    "# Define the custom transformations using the computed statistics\n",
    "cube_transform = NormalizeCube(mean=channel_means_cube.tolist(), std=channel_stds_cube.tolist())\n",
    "profile_transform = NormalizeProfile(mean=channel_means_profile.tolist(), std=channel_stds_profile.tolist())\n",
    "\n",
    "# Apply the custom transforms to the datasets\n",
    "train_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_train', cube_transform=cube_transform, profile_transform=profile_transform)\n",
    "val_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_validation', cube_transform=cube_transform, profile_transform=profile_transform)\n",
    "test_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_test', cube_transform=cube_transform, profile_transform=profile_transform)\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4795d9f6-ad04-426c-82c3-64aeab302962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of splits matches the length of the main dataset.\n",
      "No overlaps found between train and validation datasets.\n",
      "No overlaps found between train and test datasets.\n",
      "No overlaps found between validation and test datasets.\n"
     ]
    }
   ],
   "source": [
    "# Get file paths for the main dataset and each split\n",
    "main_dataset_paths = get_all_file_paths(dataset)\n",
    "train_dataset_paths = get_all_file_paths(train_dataset)\n",
    "val_dataset_paths = get_all_file_paths(val_dataset)\n",
    "test_dataset_paths = get_all_file_paths(test_dataset)\n",
    "\n",
    "# Check if the total length of the splits equals the length of the main dataset\n",
    "total_split_length = len(train_dataset_paths) + len(val_dataset_paths) + len(test_dataset_paths)\n",
    "if total_split_length != len(main_dataset_paths):\n",
    "    print(f\"Error: Total length of splits ({total_split_length}) does not equal length of main dataset ({len(main_dataset_paths)})\")\n",
    "else:\n",
    "    print(\"Total length of splits matches the length of the main dataset.\")\n",
    "\n",
    "# Check for overlaps between splits\n",
    "def check_for_overlaps(paths1, paths2):\n",
    "    overlaps = set(paths1) & set(paths2)\n",
    "    return overlaps\n",
    "\n",
    "train_val_overlaps = check_for_overlaps(train_dataset_paths, val_dataset_paths)\n",
    "train_test_overlaps = check_for_overlaps(train_dataset_paths, test_dataset_paths)\n",
    "val_test_overlaps = check_for_overlaps(val_dataset_paths, test_dataset_paths)\n",
    "\n",
    "if train_val_overlaps:\n",
    "    print(f\"Error: Overlaps found between train and validation datasets: {len(train_val_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between train and validation datasets.\")\n",
    "\n",
    "if train_test_overlaps:\n",
    "    print(f\"Error: Overlaps found between train and test datasets: {len(train_test_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between train and test datasets.\")\n",
    "\n",
    "if val_test_overlaps:\n",
    "    print(f\"Error: Overlaps found between validation and test datasets: {len(val_test_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between validation and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ffc9f-5658-4b20-9b3d-97ac24c066dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7848eefb-c25f-47f8-8344-d7f9c30ff2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the modified ResNet model\n",
    "#model = ResNet50(num_classes=len(dataset.label_map), channels=31).to(device)  # Move model to GPU\n",
    "model = CustomResNet50(num_classes=len(dataset.label_map)).to(device)\n",
    "# model = pNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f350094-d65d-4d20-9550-32cce9d0c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34fc8890-bf14-4061-8099-b5b8e9c5b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 4.0403, Training Accuracy: 36.94%\n",
      "Validation Loss: 2.2204, Validation Accuracy: 52.63%\n",
      "Epoch [2/200], Loss: 3.5218, Training Accuracy: 46.27%\n",
      "Validation Loss: 2.7742, Validation Accuracy: 63.16%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ifile/nn/ResNet-PyTorch/ResNet/venvres/lib/python3.9/site-packages/torch/optim/optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    378\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    382\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/ifile/nn/ResNet-PyTorch/ResNet/venvres/lib/python3.9/site-packages/torch/autograd/profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses, accuracy, and learning rate change epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "lr_change_epochs = []\n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 200  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (cubes, _, labels) in enumerate(train_loader):\n",
    "        cubes, labels = cubes.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(cubes)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for cubes, _, labels in val_loader:\n",
    "            cubes, labels = cubes.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(cubes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < current_lr:\n",
    "        print(f\"Learning rate reduced from {current_lr} to {new_lr}\")\n",
    "        lr_change_epochs.append(epoch+1)  # Log the epoch where the learning rate changed\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940e7b1-0479-4aaa-ba22-9bcddc094ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses with learning rate change points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "for lr_epoch in lr_change_epochs:\n",
    "    plt.axvline(x=lr_epoch, color='r', linestyle='--', label='LR Change' if lr_epoch == lr_change_epochs[0] else \"\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses with Learning Rate Changes')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "for lr_epoch in lr_change_epochs:\n",
    "    plt.axvline(x=lr_epoch, color='r', linestyle='--', label='LR Change' if lr_epoch == lr_change_epochs[0] else \"\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcfaec-3c92-48ec-8db6-db238994a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713b96c-07e1-4684-8be7-32d7237d330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test an individual sample from the test split\n",
    "def test_individual_test_sample(sample_idx):\n",
    "    # Load a specific sample from the test dataset\n",
    "    cube, label = test_dataset[sample_idx]\n",
    "    \n",
    "    # Move the sample to GPU if using CUDA\n",
    "    cube = cube.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    label = label.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        output = model(cube)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Predicted class: {predicted_class.item()}, Probability: {probabilities[0][predicted_class.item()]:.4f}\")\n",
    "    print(f\"True label: {label.item()}\")\n",
    "\n",
    "# Test an individual sample from the test split (e.g., sample index 0)\n",
    "\n",
    "for i in range(77):\n",
    "    test_individual_test_sample(i)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9776e3-04f4-4284-92a5-1a3c21285ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680598d-3493-465e-a178-c6671d8789f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa40e29-9da0-4aae-8643-4ff7f62eebde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4bd7c-c533-4da8-ae2e-01e0592b688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvres)",
   "language": "python",
   "name": "venvres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
