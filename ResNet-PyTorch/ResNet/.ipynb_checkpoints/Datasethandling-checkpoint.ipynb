{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f917bdb6-118b-4a25-ac24-b150ff6c72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import scipy.io\n",
    "import torch.nn as nn  # Import nn module\n",
    "import torch.optim as optim  # Import optim module\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ResNet import CustomResNet50  # Assuming ResNet50 is defined in ResNet.py\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c172e4-ee83-47e4-aba8-efca02e99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5] * 31, std=[0.5] * 31)  # Normalize each channel with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the .mat files, organized by species.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_map = {}\n",
    "        self.transform = transform\n",
    "\n",
    "        species_dirs = os.listdir(root_dir)\n",
    "        for idx, species in enumerate(species_dirs):\n",
    "            species_dir = os.path.join(root_dir, species)\n",
    "            if os.path.isdir(species_dir):\n",
    "                self.label_map[species] = idx\n",
    "                mat_files = [f for f in os.listdir(species_dir) if f.endswith('.mat')]\n",
    "                for mat_file in mat_files:\n",
    "                    file_path = os.path.join(species_dir, mat_file)\n",
    "                    self.data.append(file_path)\n",
    "                    self.labels.append(idx)\n",
    "        \n",
    "        print(\"Label mapping:\", self.label_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mat_file = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load .mat file\n",
    "        mat_contents = scipy.io.loadmat(mat_file)\n",
    "        cube = mat_contents['cube']\n",
    "        profile = mat_contents['profile']\n",
    "\n",
    "        # Convert to PyTorch tensor and permute dimensions\n",
    "        cube = torch.tensor(cube, dtype=torch.float32).permute(2, 0, 1)\n",
    "        profile = torch.tensor(profile, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            cube = self.transform(cube)\n",
    "\n",
    "        # return profile, label\n",
    "        return cube, label#, profile\n",
    "\n",
    "    def get_image_path(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccab103-2289-4111-bf41-0447ae956224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n",
      "Label mapping: {'Acinetobacter_baumanii': 0, 'Lactobacillus_casei': 1, 'Lactobacillus_jehnsenii': 2, 'Lactobacillus_gasseri': 3, 'Lactobacillus_delbrueckii': 4, 'Lactobacillus_salivarius': 5, 'Propionibacterium_acnes': 6, 'Lactobacillus_paracasei': 7, 'Bifidobacterium_spp': 8, 'Lactobacillus_reuteri': 9, 'Staphylococcus_aureus': 10, 'Escherichia_coli': 11, 'Lactobacillus_rhamnosus': 12, 'Lactobacillus_plantarum': 13, 'Enterococcus_faecium': 14, 'Porfyromonas_gingivalis': 15, 'Enterococcus_faecalis': 16, 'Micrococcus_spp': 17, 'Fusobacterium': 18, 'Actinomyces_israeli': 19, 'Lactobacillus_crispatus': 20, 'Clostridium_perfringens': 21, 'Staphylococcus_epidermidis': 22, 'Listeria_monocytogenes': 23, 'Pseudomonas_aeruginosa': 24, 'Lactobacillus_johnsonii': 25, 'Streptococcus_agalactiae': 26, 'Staphylococcus_saprophiticus': 27, 'Bacteroides_fragilis': 28, 'Neisseria_gonorrhoeae': 29, 'Veionella': 30, 'Proteus': 31, 'Candida_albicans': 32}\n"
     ]
    }
   ],
   "source": [
    "# Function to get all file paths from a dataset\n",
    "def get_all_file_paths(dataset):\n",
    "    file_paths = []\n",
    "    for idx in range(len(dataset)):\n",
    "        file_paths.append(dataset.get_image_path(idx))\n",
    "    return file_paths\n",
    "\n",
    "# # Initialize the dataset with transformations\n",
    "dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_all', transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_train', transform=transform)\n",
    "test_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_test', transform=transform)\n",
    "val_dataset = HyperspectralDataset(root_dir='../../resizedstuff/mst_resized_validation', transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4795d9f6-ad04-426c-82c3-64aeab302962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of splits matches the length of the main dataset.\n",
      "No overlaps found between train and validation datasets.\n",
      "No overlaps found between train and test datasets.\n",
      "No overlaps found between validation and test datasets.\n"
     ]
    }
   ],
   "source": [
    "# Get file paths for the main dataset and each split\n",
    "main_dataset_paths = get_all_file_paths(dataset)\n",
    "train_dataset_paths = get_all_file_paths(train_dataset)\n",
    "val_dataset_paths = get_all_file_paths(val_dataset)\n",
    "test_dataset_paths = get_all_file_paths(test_dataset)\n",
    "\n",
    "# Check if the total length of the splits equals the length of the main dataset\n",
    "total_split_length = len(train_dataset_paths) + len(val_dataset_paths) + len(test_dataset_paths)\n",
    "if total_split_length != len(main_dataset_paths):\n",
    "    print(f\"Error: Total length of splits ({total_split_length}) does not equal length of main dataset ({len(main_dataset_paths)})\")\n",
    "else:\n",
    "    print(\"Total length of splits matches the length of the main dataset.\")\n",
    "\n",
    "# Check for overlaps between splits\n",
    "def check_for_overlaps(paths1, paths2):\n",
    "    overlaps = set(paths1) & set(paths2)\n",
    "    return overlaps\n",
    "\n",
    "train_val_overlaps = check_for_overlaps(train_dataset_paths, val_dataset_paths)\n",
    "train_test_overlaps = check_for_overlaps(train_dataset_paths, test_dataset_paths)\n",
    "val_test_overlaps = check_for_overlaps(val_dataset_paths, test_dataset_paths)\n",
    "\n",
    "if train_val_overlaps:\n",
    "    print(f\"Error: Overlaps found between train and validation datasets: {len(train_val_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between train and validation datasets.\")\n",
    "\n",
    "if train_test_overlaps:\n",
    "    print(f\"Error: Overlaps found between train and test datasets: {len(train_test_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between train and test datasets.\")\n",
    "\n",
    "if val_test_overlaps:\n",
    "    print(f\"Error: Overlaps found between validation and test datasets: {len(val_test_overlaps)} overlaps\")\n",
    "else:\n",
    "    print(\"No overlaps found between validation and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ffc9f-5658-4b20-9b3d-97ac24c066dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7848eefb-c25f-47f8-8344-d7f9c30ff2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the modified ResNet model\n",
    "#model = ResNet50(num_classes=len(dataset.label_map), channels=31).to(device)  # Move model to GPU\n",
    "model = CustomResNet50(num_classes=len(dataset.label_map)).to(device)\n",
    "# model = pNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f350094-d65d-4d20-9550-32cce9d0c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fc8890-bf14-4061-8099-b5b8e9c5b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 5.6921, Training Accuracy: 18.47%   Shape: torch.Size([4, 33])\n",
      "Validation Loss: 4.3604, Validation Accuracy: 31.58%\n",
      "Epoch [2/200], Loss: 4.0803, Training Accuracy: 37.13%   Shape: torch.Size([4, 33])\n",
      "Validation Loss: 3.8538, Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move data to GPU\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/Desktop/ifile/nn/ResNet-PyTorch/ResNet/venvres/lib/python3.9/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ifile/nn/ResNet-PyTorch/ResNet/venvres/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Desktop/ifile/nn/ResNet-PyTorch/ResNet/venvres/lib/python3.9/site-packages/torch/optim/optimizer.py:823\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 823\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m    825\u001b[0m                 p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses, accuracy, and learning rate change epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "lr_change_epochs = []\n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 200  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < current_lr:\n",
    "        print(f\"Learning rate reduced from {current_lr} to {new_lr}\")\n",
    "        lr_change_epochs.append(epoch+1)  # Log the epoch where the learning rate changed\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940e7b1-0479-4aaa-ba22-9bcddc094ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses with learning rate change points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "for lr_epoch in lr_change_epochs:\n",
    "    plt.axvline(x=lr_epoch, color='r', linestyle='--', label='LR Change' if lr_epoch == lr_change_epochs[0] else \"\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses with Learning Rate Changes')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "for lr_epoch in lr_change_epochs:\n",
    "    plt.axvline(x=lr_epoch, color='r', linestyle='--', label='LR Change' if lr_epoch == lr_change_epochs[0] else \"\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcfaec-3c92-48ec-8db6-db238994a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713b96c-07e1-4684-8be7-32d7237d330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test an individual sample from the test split\n",
    "def test_individual_test_sample(sample_idx):\n",
    "    # Load a specific sample from the test dataset\n",
    "    cube, label = test_dataset[sample_idx]\n",
    "    \n",
    "    # Move the sample to GPU if using CUDA\n",
    "    cube = cube.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    label = label.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        output = model(cube)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Predicted class: {predicted_class.item()}, Probability: {probabilities[0][predicted_class.item()]:.4f}\")\n",
    "    print(f\"True label: {label.item()}\")\n",
    "\n",
    "# Test an individual sample from the test split (e.g., sample index 0)\n",
    "test_individual_test_sample(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9776e3-04f4-4284-92a5-1a3c21285ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680598d-3493-465e-a178-c6671d8789f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa40e29-9da0-4aae-8643-4ff7f62eebde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4bd7c-c533-4da8-ae2e-01e0592b688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvres)",
   "language": "python",
   "name": "venvres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
